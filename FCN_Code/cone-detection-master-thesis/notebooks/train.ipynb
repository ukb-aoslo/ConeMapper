{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train DT network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average running TRAINING loss for epoch 1: 5.704558099138326\n",
      "Average running VALIDATION loss for epoch 1: 5.878391209244728\n",
      "Average running TRAINING loss for epoch 2: 2.24878742854143\n",
      "Average running VALIDATION loss for epoch 2: 2.6218142323195934\n",
      "Average running TRAINING loss for epoch 3: 0.7865463591598231\n",
      "Average running VALIDATION loss for epoch 3: 2.1108170792460443\n",
      "Average running TRAINING loss for epoch 4: 0.5782917836873696\n",
      "Average running VALIDATION loss for epoch 4: 1.9916305169463158\n",
      "Average running TRAINING loss for epoch 5: 0.587471532539047\n",
      "Average running VALIDATION loss for epoch 5: 2.003329253941774\n",
      "Average running TRAINING loss for epoch 6: 0.5537111788731197\n",
      "Average running VALIDATION loss for epoch 6: 2.0273272044956685\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mf:\\Repositories\\cone-detection-master-thesis\\notebooks\\train.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Repositories/cone-detection-master-thesis/notebooks/train.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m fcn \u001b[39m=\u001b[39m FCN(train_dataloader, validation_dataloader\u001b[39m=\u001b[39mvalidation_dataloader, lr\u001b[39m=\u001b[39m\u001b[39m3e-4\u001b[39m, depth\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, input_channels\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, initial_feature_maps\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, blocks_per_resolution_layer\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, use_MSE_loss\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Repositories/cone-detection-master-thesis/notebooks/train.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m#fcn = FCN(train_dataloader, validation_dataloader=validation_dataloader, lr=3e-4, depth=3, input_channels=1, initial_feature_maps=32, blocks_per_resolution_layer=3, use_MSE_loss=True)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Repositories/cone-detection-master-thesis/notebooks/train.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Repositories/cone-detection-master-thesis/notebooks/train.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#fcn = FCN(train_dataloader, validation_dataloader=validation_dataloader, lr=3e-4, depth=3, input_channels=1, initial_feature_maps=48, blocks_per_resolution_layer=2, use_MSE_loss=True)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/Repositories/cone-detection-master-thesis/notebooks/train.ipynb#W2sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m fcn\u001b[39m.\u001b[39;49mtrain_network(epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "File \u001b[1;32mf:\\Repositories\\cone-detection-master-thesis\\notebooks\\..\\components\\fcn.py:230\u001b[0m, in \u001b[0;36mFCN.train_network\u001b[1;34m(self, epochs)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m    229\u001b[0m     running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m--> 230\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataloader:\n\u001b[0;32m    231\u001b[0m         imgs, labels, identifiers \u001b[39m=\u001b[39m data\n\u001b[0;32m    232\u001b[0m         \u001b[39m#imgs, labels, gradients_y, gradients_x = data\u001b[39;00m\n\u001b[0;32m    233\u001b[0m         \u001b[39m#print(imgs.shape, labels.shape)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mf:\\Repositories\\cone-detection-master-thesis\\notebooks\\..\\components\\datasets.py:107\u001b[0m, in \u001b[0;36mConesDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    101\u001b[0m     img, label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrotate(img, label)\n\u001b[0;32m    103\u001b[0m     \u001b[39m# Random blur\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     \u001b[39m#img = self.blur(img)\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m     \u001b[39m# Random elastic distortion\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m     img, label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49melastic(img, label)\n\u001b[0;32m    109\u001b[0m \u001b[39m# Always apply auto contrast\u001b[39;00m\n\u001b[0;32m    110\u001b[0m img \u001b[39m=\u001b[39m TF\u001b[39m.\u001b[39mautocontrast(img)\n",
      "File \u001b[1;32mf:\\Repositories\\cone-detection-master-thesis\\notebooks\\..\\utils\\transforms.py:77\u001b[0m, in \u001b[0;36mCustomElasticTransform.__call__\u001b[1;34m(self, *arrays)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39marrays):\n\u001b[0;32m     76\u001b[0m     img_height, img_width \u001b[39m=\u001b[39m arrays[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mshape\n\u001b[1;32m---> 77\u001b[0m     displacement \u001b[39m=\u001b[39m ElasticTransform\u001b[39m.\u001b[39;49mget_params(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malpha, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msigma, [img_height, img_width])\n\u001b[0;32m     78\u001b[0m     \u001b[39mreturn\u001b[39;00m [TF\u001b[39m.\u001b[39melastic_transform(array, displacement, InterpolationMode\u001b[39m.\u001b[39mBILINEAR, \u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m array \u001b[39min\u001b[39;00m arrays]\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torchvision\\transforms\\transforms.py:2119\u001b[0m, in \u001b[0;36mElasticTransform.get_params\u001b[1;34m(alpha, sigma, size)\u001b[0m\n\u001b[0;32m   2117\u001b[0m     \u001b[39mif\u001b[39;00m ky \u001b[39m%\u001b[39m \u001b[39m2\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   2118\u001b[0m         ky \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 2119\u001b[0m     dy \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mgaussian_blur(dy, [ky, ky], sigma)\n\u001b[0;32m   2120\u001b[0m dy \u001b[39m=\u001b[39m dy \u001b[39m*\u001b[39m alpha[\u001b[39m1\u001b[39m] \u001b[39m/\u001b[39m size[\u001b[39m1\u001b[39m]\n\u001b[0;32m   2121\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mconcat([dx, dy], \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mpermute([\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torchvision\\transforms\\functional.py:1366\u001b[0m, in \u001b[0;36mgaussian_blur\u001b[1;34m(img, kernel_size, sigma)\u001b[0m\n\u001b[0;32m   1362\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be PIL Image or Tensor. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(img)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1364\u001b[0m     t_img \u001b[39m=\u001b[39m pil_to_tensor(img)\n\u001b[1;32m-> 1366\u001b[0m output \u001b[39m=\u001b[39m F_t\u001b[39m.\u001b[39;49mgaussian_blur(t_img, kernel_size, sigma)\n\u001b[0;32m   1368\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m   1369\u001b[0m     output \u001b[39m=\u001b[39m to_pil_image(output, mode\u001b[39m=\u001b[39mimg\u001b[39m.\u001b[39mmode)\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:768\u001b[0m, in \u001b[0;36mgaussian_blur\u001b[1;34m(img, kernel_size, sigma)\u001b[0m\n\u001b[0;32m    766\u001b[0m padding \u001b[39m=\u001b[39m [kernel_size[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m, kernel_size[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m, kernel_size[\u001b[39m1\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m, kernel_size[\u001b[39m1\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m]\n\u001b[0;32m    767\u001b[0m img \u001b[39m=\u001b[39m torch_pad(img, padding, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreflect\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 768\u001b[0m img \u001b[39m=\u001b[39m conv2d(img, kernel, groups\u001b[39m=\u001b[39;49mimg\u001b[39m.\u001b[39;49mshape[\u001b[39m-\u001b[39;49m\u001b[39m3\u001b[39;49m])\n\u001b[0;32m    770\u001b[0m img \u001b[39m=\u001b[39m _cast_squeeze_out(img, need_cast, need_squeeze, out_dtype)\n\u001b[0;32m    771\u001b[0m \u001b[39mreturn\u001b[39;00m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from components.fcn import FCN\n",
    "from components.datasets import ConesDataset\n",
    "\n",
    "dataset_name = \"dataset_no_cross_offline\"\n",
    "\n",
    "train_dataset = ConesDataset(f\"F:/Data/ConeDatasets/{dataset_name}/train\", train=True, convert_to_dt=True, pre_generated=True, augment_rotate=True)\n",
    "train_dataloader = train_dataset.get_dataloader(shuffle=True,batch_size=32)\n",
    "\n",
    "validation_dataset = ConesDataset(f\"F:/Data/ConeDatasets/{dataset_name}/validation\", train=False, convert_to_dt=True, pre_generated=True)\n",
    "validation_dataloader = validation_dataset.get_dataloader(shuffle=False, batch_size=32)\n",
    "\n",
    "#fcn = FCN(train_dataloader, validation_dataloader=validation_dataloader, lr=3e-4, depth=3, input_channels=1, initial_feature_maps=16, blocks_per_resolution_layer=2, use_MSE_loss=True)\n",
    "#fcn = FCN(train_dataloader, validation_dataloader=validation_dataloader, lr=3e-4, depth=3, input_channels=1, initial_feature_maps=16, blocks_per_resolution_layer=4, use_MSE_loss=True)\n",
    "\n",
    "fcn = FCN(train_dataloader, validation_dataloader=validation_dataloader, lr=3e-4, depth=3, input_channels=1, \n",
    "    initial_feature_maps=32, blocks_per_resolution_layer=2, use_MSE_loss=True, masked_MSE_loss=True)\n",
    "#fcn = FCN(train_dataloader, validation_dataloader=validation_dataloader, lr=3e-4, depth=3, input_channels=1, initial_feature_maps=32, blocks_per_resolution_layer=3, use_MSE_loss=True)\n",
    "\n",
    "#fcn = FCN(train_dataloader, validation_dataloader=validation_dataloader, lr=3e-4, depth=3, input_channels=1, initial_feature_maps=48, blocks_per_resolution_layer=2, use_MSE_loss=True)\n",
    "\n",
    "fcn.train_network(epochs=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train DT network with k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\cones\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  8%|▊         | 25/300 [2:37:29<28:52:20, 377.97s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mf:\\Repositories\\cone-detection-master-thesis\\notebooks\\train.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Repositories/cone-detection-master-thesis/notebooks/train.ipynb#W3sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m dataloaders \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mget_dataloaders_k_fold_cross_validation(k, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m) \u001b[39m#48)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Repositories/cone-detection-master-thesis/notebooks/train.ipynb#W3sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m#FCN.train_networks_k_fold_cross_validation(dataloaders, max_epochs=epochs_per_fold, early_stopping=True)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/Repositories/cone-detection-master-thesis/notebooks/train.ipynb#W3sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m FCN\u001b[39m.\u001b[39;49mtrain_networks_k_fold_cross_validation(dataloaders, max_epochs\u001b[39m=\u001b[39;49mepochs_per_fold, lr\u001b[39m=\u001b[39;49m\u001b[39m3e-4\u001b[39;49m, early_stopping\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m#True, \u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Repositories/cone-detection-master-thesis/notebooks/train.ipynb#W3sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m                                            use_lr_scheduler\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mf:\\Repositories\\cone-detection-master-thesis\\notebooks\\..\\components\\fcn.py:462\u001b[0m, in \u001b[0;36mFCN.train_networks_k_fold_cross_validation\u001b[1;34m(dataloaders, max_epochs, lr, early_stopping, regularity_aware_training, use_distance_information, use_refiner, use_lr_scheduler)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[39mfor\u001b[39;00m k, (train_dataloader, validation_dataloader) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloaders):\n\u001b[0;32m    457\u001b[0m     model \u001b[39m=\u001b[39m FCN(train_dataloader, validation_dataloader\u001b[39m=\u001b[39mvalidation_dataloader, lr\u001b[39m=\u001b[39mlr, depth\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, input_channels\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, \n\u001b[0;32m    458\u001b[0m         initial_feature_maps\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, blocks_per_resolution_layer\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, use_MSE_loss\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, masked_MSE_loss\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, use_class_weights\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    459\u001b[0m         regularity_aware_training\u001b[39m=\u001b[39mregularity_aware_training, use_distance_information\u001b[39m=\u001b[39muse_distance_information, use_refiner\u001b[39m=\u001b[39muse_refiner,\n\u001b[0;32m    460\u001b[0m         use_lr_scheduler\u001b[39m=\u001b[39muse_lr_scheduler)\n\u001b[1;32m--> 462\u001b[0m     trained_epochs, min_training_loss, min_validation_loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_network(\n\u001b[0;32m    463\u001b[0m         max_epochs\u001b[39m=\u001b[39;49mmax_epochs, \n\u001b[0;32m    464\u001b[0m         run_id\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfold-\u001b[39;49m\u001b[39m{\u001b[39;49;00mk\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, \n\u001b[0;32m    465\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \n\u001b[0;32m    466\u001b[0m         early_stopping\u001b[39m=\u001b[39;49mearly_stopping)\n\u001b[0;32m    468\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFold \u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mtrained_epochs\u001b[39m}\u001b[39;00m\u001b[39m epochs, (\u001b[39m\u001b[39m{\u001b[39;00mmin_training_loss\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00mmin_validation_loss\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    469\u001b[0m     \u001b[39mdel\u001b[39;00m model\n",
      "File \u001b[1;32mf:\\Repositories\\cone-detection-master-thesis\\notebooks\\..\\components\\fcn.py:289\u001b[0m, in \u001b[0;36mFCN.train_network\u001b[1;34m(self, max_epochs, run_id, verbose, early_stopping)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39mfor\u001b[39;00m current_epoch \u001b[39min\u001b[39;00m iterator: \u001b[39m#range(epochs):\u001b[39;00m\n\u001b[0;32m    288\u001b[0m     running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m--> 289\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataloader:\n\u001b[0;32m    290\u001b[0m         imgs, labels, identifiers, cdc, distances \u001b[39m=\u001b[39m data\n\u001b[0;32m    292\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mf:\\Repositories\\cone-detection-master-thesis\\notebooks\\..\\components\\datasets.py:136\u001b[0m, in \u001b[0;36mConesDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    133\u001b[0m     label \u001b[39m=\u001b[39m new_label\n\u001b[0;32m    135\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 136\u001b[0m     label \u001b[39m=\u001b[39m ndi\u001b[39m.\u001b[39;49mbinary_dilation(label, iterations\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation)\n\u001b[0;32m    138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvert_to_dt:\n\u001b[0;32m    139\u001b[0m     label \u001b[39m=\u001b[39m class_to_dt(label)\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\scipy\\ndimage\\_morphology.py:517\u001b[0m, in \u001b[0;36mbinary_dilation\u001b[1;34m(input, structure, iterations, mask, output, border_value, origin, brute_force)\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m structure\u001b[39m.\u001b[39mshape[ii] \u001b[39m&\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    515\u001b[0m         origin[ii] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 517\u001b[0m \u001b[39mreturn\u001b[39;00m _binary_erosion(\u001b[39minput\u001b[39;49m, structure, iterations, mask,\n\u001b[0;32m    518\u001b[0m                        output, border_value, origin, \u001b[39m1\u001b[39;49m, brute_force)\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\scipy\\ndimage\\_morphology.py:253\u001b[0m, in \u001b[0;36m_binary_erosion\u001b[1;34m(input, structure, iterations, mask, output, border_value, origin, invert, brute_force)\u001b[0m\n\u001b[0;32m    251\u001b[0m     output \u001b[39m=\u001b[39m _ni_support\u001b[39m.\u001b[39m_get_output(output\u001b[39m.\u001b[39mdtype, \u001b[39minput\u001b[39m)\n\u001b[0;32m    252\u001b[0m \u001b[39mif\u001b[39;00m iterations \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 253\u001b[0m     _nd_image\u001b[39m.\u001b[39;49mbinary_erosion(\u001b[39minput\u001b[39;49m, structure, mask, output,\n\u001b[0;32m    254\u001b[0m                              border_value, origin, invert, cit, \u001b[39m0\u001b[39;49m)\n\u001b[0;32m    255\u001b[0m \u001b[39melif\u001b[39;00m cit \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m brute_force:\n\u001b[0;32m    256\u001b[0m     changed, coordinate_list \u001b[39m=\u001b[39m _nd_image\u001b[39m.\u001b[39mbinary_erosion(\n\u001b[0;32m    257\u001b[0m         \u001b[39minput\u001b[39m, structure, mask, output,\n\u001b[0;32m    258\u001b[0m         border_value, origin, invert, cit, \u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from components.fcn import FCN\n",
    "from components.datasets import ConesDataset\n",
    "\n",
    "# Config\n",
    "k = 8 #5\n",
    "epochs_per_fold = 300 #50 #25 #30 #50\n",
    "dataset_name = \"20230301_cross_eroded_equalized_64_cropped\"\n",
    "#\"20230301_cross_eroded_equalized_cropped\" #\"20230301_cross_eroded_cropped\"\n",
    "#\"cross_excluded_eroded_144_offline_added\"\n",
    "#\"dataset_cross_no_BAK8095L_eroded_offline_low_plus\" #\"dataset_cross_no_BAK8095L_eroded_offline_512\" #\"dataset_cross_offline_eroded\" # \"dataset_cross_offline\"\n",
    "\n",
    "# Create dataset\n",
    "dataset = ConesDataset(f\"F:/Data/ConeDatasets/{dataset_name}/train\", train=True, \n",
    "    convert_to_dt=True, #False,\n",
    "    convert_to_gaussians=False, #True,\n",
    "    pre_generated=True, \n",
    "    augment_rotate=True, \n",
    "    augment_blur=False, #True\n",
    "    augment_elastic=False,\n",
    "    adaptive_dilation=False,\n",
    "    dilation = 1) \n",
    "\n",
    "# Create folds and save to disk (ONLY USE ONCE)\n",
    "# dataset.get_k_fold_splits(k, \"F:/Repositories/cone-detection-master-thesis/folds\")\n",
    "\n",
    "# # Train with a SINGLE fold\n",
    "# fold_id = 0\n",
    "# fold_path = f\"F:/Repositories/cone-detection-master-thesis/folds/fold_{fold_id}.npz\"\n",
    "# train_dataloader, validation_dataloader = dataset.get_single_dataloader_cross_validation(fold_path, batch_size=32)\n",
    "# FCN.train_network_single_cross_validation(fold_id, train_dataloader, validation_dataloader, max_epochs=epochs_per_fold, early_stopping=True)\n",
    "\n",
    "# Train WITH all k folds\n",
    "dataloaders = dataset.get_dataloaders_k_fold_cross_validation(k, batch_size=32) #48)\n",
    "#FCN.train_networks_k_fold_cross_validation(dataloaders, max_epochs=epochs_per_fold, early_stopping=True)\n",
    "FCN.train_networks_k_fold_cross_validation(dataloaders, max_epochs=epochs_per_fold, lr=3e-4, early_stopping=False, #True, \n",
    "                                           use_lr_scheduler=True) #,\n",
    "#                                            use_distance_information=False, regularity_aware_training=False,\n",
    "#                                            use_refiner=False) #True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train DT network without validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\cones\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      " 47%|████▋     | 141/300 [14:41:13<16:33:43, 374.99s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mf:\\Repositories\\cone-detection-master-thesis\\notebooks\\train.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Repositories/cone-detection-master-thesis/notebooks/train.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m model \u001b[39m=\u001b[39m FCN(dataloader, lr\u001b[39m=\u001b[39m\u001b[39m1e-5\u001b[39m, depth\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, input_channels\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Repositories/cone-detection-master-thesis/notebooks/train.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m             initial_feature_maps\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, blocks_per_resolution_layer\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Repositories/cone-detection-master-thesis/notebooks/train.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m             use_MSE_loss\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, masked_MSE_loss\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, use_class_weights\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Repositories/cone-detection-master-thesis/notebooks/train.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m             use_lr_scheduler\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m#, lr=3e-4)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Repositories/cone-detection-master-thesis/notebooks/train.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m model\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39m../nets/DT-0.16-dilation-1-300-epochs.pth\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/Repositories/cone-detection-master-thesis/notebooks/train.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m model\u001b[39m.\u001b[39;49mtrain_network(max_epochs\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m, run_id\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtransfer-DT-0.16\u001b[39;49m\u001b[39m\"\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, early_stopping\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mf:\\Repositories\\cone-detection-master-thesis\\notebooks\\..\\components\\fcn.py:289\u001b[0m, in \u001b[0;36mFCN.train_network\u001b[1;34m(self, max_epochs, run_id, verbose, early_stopping)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39mfor\u001b[39;00m current_epoch \u001b[39min\u001b[39;00m iterator: \u001b[39m#range(epochs):\u001b[39;00m\n\u001b[0;32m    288\u001b[0m     running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m--> 289\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataloader:\n\u001b[0;32m    290\u001b[0m         imgs, labels, identifiers, cdc, distances \u001b[39m=\u001b[39m data\n\u001b[0;32m    292\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mf:\\Repositories\\cone-detection-master-thesis\\notebooks\\..\\components\\datasets.py:154\u001b[0m, in \u001b[0;36mConesDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain:\n\u001b[0;32m    152\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maugment_rotate:\n\u001b[0;32m    153\u001b[0m         \u001b[39m# Random rotation\u001b[39;00m\n\u001b[1;32m--> 154\u001b[0m         img, label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrotate(img, label)\n\u001b[0;32m    156\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maugment_blur:\n\u001b[0;32m    157\u001b[0m         \u001b[39m# Random blur\u001b[39;00m\n\u001b[0;32m    158\u001b[0m         img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblur(img)\n",
      "File \u001b[1;32mf:\\Repositories\\cone-detection-master-thesis\\notebooks\\..\\utils\\transforms.py:22\u001b[0m, in \u001b[0;36mRotationTransform.__call__\u001b[1;34m(self, *arrays)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39marrays):\n\u001b[0;32m     21\u001b[0m     angle \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mchoice(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mangles)\n\u001b[1;32m---> 22\u001b[0m     \u001b[39mreturn\u001b[39;00m [TF\u001b[39m.\u001b[39mrotate(array, angle) \u001b[39mfor\u001b[39;00m array \u001b[39min\u001b[39;00m arrays]\n",
      "File \u001b[1;32mf:\\Repositories\\cone-detection-master-thesis\\notebooks\\..\\utils\\transforms.py:22\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39marrays):\n\u001b[0;32m     21\u001b[0m     angle \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mchoice(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mangles)\n\u001b[1;32m---> 22\u001b[0m     \u001b[39mreturn\u001b[39;00m [TF\u001b[39m.\u001b[39;49mrotate(array, angle) \u001b[39mfor\u001b[39;00m array \u001b[39min\u001b[39;00m arrays]\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torchvision\\transforms\\functional.py:1118\u001b[0m, in \u001b[0;36mrotate\u001b[1;34m(img, angle, interpolation, expand, center, fill)\u001b[0m\n\u001b[0;32m   1115\u001b[0m \u001b[39m# due to current incoherence of rotation angle direction between affine and rotate implementations\u001b[39;00m\n\u001b[0;32m   1116\u001b[0m \u001b[39m# we need to set -angle.\u001b[39;00m\n\u001b[0;32m   1117\u001b[0m matrix \u001b[39m=\u001b[39m _get_inverse_affine_matrix(center_f, \u001b[39m-\u001b[39mangle, [\u001b[39m0.0\u001b[39m, \u001b[39m0.0\u001b[39m], \u001b[39m1.0\u001b[39m, [\u001b[39m0.0\u001b[39m, \u001b[39m0.0\u001b[39m])\n\u001b[1;32m-> 1118\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49mrotate(img, matrix\u001b[39m=\u001b[39;49mmatrix, interpolation\u001b[39m=\u001b[39;49minterpolation\u001b[39m.\u001b[39;49mvalue, expand\u001b[39m=\u001b[39;49mexpand, fill\u001b[39m=\u001b[39;49mfill)\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:671\u001b[0m, in \u001b[0;36mrotate\u001b[1;34m(img, matrix, interpolation, expand, fill)\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[39m# grid will be generated on the same device as theta and img\u001b[39;00m\n\u001b[0;32m    669\u001b[0m grid \u001b[39m=\u001b[39m _gen_affine_grid(theta, w\u001b[39m=\u001b[39mw, h\u001b[39m=\u001b[39mh, ow\u001b[39m=\u001b[39mow, oh\u001b[39m=\u001b[39moh)\n\u001b[1;32m--> 671\u001b[0m \u001b[39mreturn\u001b[39;00m _apply_grid_transform(img, grid, interpolation, fill\u001b[39m=\u001b[39;49mfill)\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:562\u001b[0m, in \u001b[0;36m_apply_grid_transform\u001b[1;34m(img, grid, mode, fill)\u001b[0m\n\u001b[0;32m    559\u001b[0m     mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones((img\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m, img\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m], img\u001b[39m.\u001b[39mshape[\u001b[39m3\u001b[39m]), dtype\u001b[39m=\u001b[39mimg\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39mimg\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    560\u001b[0m     img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((img, mask), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m--> 562\u001b[0m img \u001b[39m=\u001b[39m grid_sample(img, grid, mode\u001b[39m=\u001b[39;49mmode, padding_mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mzeros\u001b[39;49m\u001b[39m\"\u001b[39;49m, align_corners\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    564\u001b[0m \u001b[39m# Fill with required color\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[39mif\u001b[39;00m fill \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torch\\nn\\functional.py:4235\u001b[0m, in \u001b[0;36mgrid_sample\u001b[1;34m(input, grid, mode, padding_mode, align_corners)\u001b[0m\n\u001b[0;32m   4227\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   4228\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDefault grid_sample and affine_grid behavior has changed \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4229\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mto align_corners=False since 1.3.0. Please specify \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4230\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39malign_corners=True if the old behavior is desired. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4231\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee the documentation of grid_sample for details.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4232\u001b[0m     )\n\u001b[0;32m   4233\u001b[0m     align_corners \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 4235\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mgrid_sampler(\u001b[39minput\u001b[39;49m, grid, mode_enum, padding_mode_enum, align_corners)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from components.fcn import FCN\n",
    "from components.datasets import ConesDataset\n",
    "\n",
    "# Config\n",
    "dataset_name = \"20230301_cross_eroded_cropped\"\n",
    "\n",
    "# Create dataset\n",
    "dataset = ConesDataset(f\"F:/Data/ConeDatasets/{dataset_name}/train\", train=True, \n",
    "    convert_to_dt=True, #False,\n",
    "    convert_to_gaussians=False, #True,\n",
    "    pre_generated=True, \n",
    "    augment_rotate=True, \n",
    "    augment_blur=False, #True\n",
    "    augment_elastic=False,\n",
    "    adaptive_dilation=False,\n",
    "    dilation = 1) \n",
    "dataloader = dataset.get_dataloader(batch_size=32)\n",
    "\n",
    "model = FCN(dataloader, lr=1e-5, depth=3, input_channels=1, \n",
    "            initial_feature_maps=32, blocks_per_resolution_layer=2, \n",
    "            use_MSE_loss=True, masked_MSE_loss=True, use_class_weights=False,\n",
    "            use_lr_scheduler=True) #, lr=3e-4)\n",
    "\n",
    "model.load(\"../nets/DT-0.16-dilation-1-300-epochs.pth\")\n",
    "\n",
    "model.train_network(max_epochs=300, run_id=\"transfer-DT-0.16\", verbose=False, early_stopping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\cones\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      " 15%|█▌        | 3/20 [03:17<18:39, 65.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 epochs, (0.3335914104073136,0.4250504585603873)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Transfer learning with low density\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\"..\")\n",
    "# import torch\n",
    "\n",
    "# from components.fcn import FCN\n",
    "# from components.datasets import ConesDataset\n",
    "\n",
    "# max_epochs = 20\n",
    "# early_stopping = True\n",
    "\n",
    "# train_dataset = ConesDataset(f\"F:/Data/ConeDatasets/confocal_npz\", train=True, convert_to_dt=True, pre_generated=True, augment_rotate=True)\n",
    "# train_dataloader = train_dataset.get_dataloader(shuffle=True,batch_size=32)\n",
    "\n",
    "# validation_dataset = ConesDataset(f\"F:/Data/ConeDatasets/dataset_cross_no_BAK8095L_eroded_offline/test\", train=False, convert_to_dt=True, pre_generated=True)\n",
    "# validation_dataloader = validation_dataset.get_dataloader(shuffle=False, batch_size=32)\n",
    "\n",
    "# model = FCN(train_dataloader, validation_dataloader=validation_dataloader, lr=3e-4, depth=3, input_channels=1, \n",
    "#                 initial_feature_maps=32, blocks_per_resolution_layer=2, use_MSE_loss=True, masked_MSE_loss=True, use_class_weights=False)\n",
    "\n",
    "# #model.load(\"../nets/5-fold-eroded-excluded-512-DT-0.23.pth\")\n",
    "            \n",
    "# trained_epochs, min_training_loss, min_validation_loss = model.train_network(\n",
    "#     max_epochs=max_epochs, \n",
    "#     run_id=f\"transfer_learning\", \n",
    "#     verbose=False, \n",
    "#     early_stopping=early_stopping)\n",
    "\n",
    "# print(f\"{trained_epochs} epochs, ({min_training_loss},{min_validation_loss})\")\n",
    "# del model\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\cones\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      " 32%|███▏      | 8/25 [56:30<2:00:04, 423.77s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mf:\\Repositories\\cone-detection-master-thesis\\notebooks\\train.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Repositories/cone-detection-master-thesis/notebooks/train.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Train WITH all k folds\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Repositories/cone-detection-master-thesis/notebooks/train.ipynb#X13sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m dataloaders \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mget_dataloaders_k_fold_cross_validation(k, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/Repositories/cone-detection-master-thesis/notebooks/train.ipynb#X13sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m Hamwood\u001b[39m.\u001b[39;49mtrain_networks_k_fold_cross_validation(dataloaders, max_epochs\u001b[39m=\u001b[39;49mepochs_per_fold, early_stopping\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mf:\\Repositories\\cone-detection-master-thesis\\notebooks\\..\\components\\hamwood.py:254\u001b[0m, in \u001b[0;36mHamwood.train_networks_k_fold_cross_validation\u001b[1;34m(dataloaders, max_epochs, early_stopping)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mfor\u001b[39;00m k, (train_dataloader, validation_dataloader) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloaders):\n\u001b[0;32m    252\u001b[0m     model \u001b[39m=\u001b[39m Hamwood(train_dataloader, validation_dataloader\u001b[39m=\u001b[39mvalidation_dataloader, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m)\n\u001b[1;32m--> 254\u001b[0m     trained_epochs, min_training_loss, min_validation_loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_network(\n\u001b[0;32m    255\u001b[0m         max_epochs\u001b[39m=\u001b[39;49mmax_epochs, \n\u001b[0;32m    256\u001b[0m         run_id\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfold-\u001b[39;49m\u001b[39m{\u001b[39;49;00mk\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, \n\u001b[0;32m    257\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \n\u001b[0;32m    258\u001b[0m         early_stopping\u001b[39m=\u001b[39;49mearly_stopping)\n\u001b[0;32m    260\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFold \u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mtrained_epochs\u001b[39m}\u001b[39;00m\u001b[39m epochs, (\u001b[39m\u001b[39m{\u001b[39;00mmin_training_loss\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m{\u001b[39;00mmin_validation_loss\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    261\u001b[0m     \u001b[39mdel\u001b[39;00m model\n",
      "File \u001b[1;32mf:\\Repositories\\cone-detection-master-thesis\\notebooks\\..\\components\\hamwood.py:151\u001b[0m, in \u001b[0;36mHamwood.train_network\u001b[1;34m(self, max_epochs, run_id, verbose, early_stopping)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[39mfor\u001b[39;00m current_epoch \u001b[39min\u001b[39;00m iterator: \u001b[39m#range(epochs):\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m--> 151\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataloader:\n\u001b[0;32m    152\u001b[0m         imgs, labels, identifiers, cdc, distances \u001b[39m=\u001b[39m data\n\u001b[0;32m    154\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mf:\\Repositories\\cone-detection-master-thesis\\notebooks\\..\\components\\datasets.py:154\u001b[0m, in \u001b[0;36mConesDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain:\n\u001b[0;32m    152\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maugment_rotate:\n\u001b[0;32m    153\u001b[0m         \u001b[39m# Random rotation\u001b[39;00m\n\u001b[1;32m--> 154\u001b[0m         img, label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrotate(img, label)\n\u001b[0;32m    156\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maugment_blur:\n\u001b[0;32m    157\u001b[0m         \u001b[39m# Random blur\u001b[39;00m\n\u001b[0;32m    158\u001b[0m         img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblur(img)\n",
      "File \u001b[1;32mf:\\Repositories\\cone-detection-master-thesis\\notebooks\\..\\utils\\transforms.py:22\u001b[0m, in \u001b[0;36mRotationTransform.__call__\u001b[1;34m(self, *arrays)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39marrays):\n\u001b[0;32m     21\u001b[0m     angle \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mchoice(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mangles)\n\u001b[1;32m---> 22\u001b[0m     \u001b[39mreturn\u001b[39;00m [TF\u001b[39m.\u001b[39mrotate(array, angle) \u001b[39mfor\u001b[39;00m array \u001b[39min\u001b[39;00m arrays]\n",
      "File \u001b[1;32mf:\\Repositories\\cone-detection-master-thesis\\notebooks\\..\\utils\\transforms.py:22\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39marrays):\n\u001b[0;32m     21\u001b[0m     angle \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mchoice(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mangles)\n\u001b[1;32m---> 22\u001b[0m     \u001b[39mreturn\u001b[39;00m [TF\u001b[39m.\u001b[39;49mrotate(array, angle) \u001b[39mfor\u001b[39;00m array \u001b[39min\u001b[39;00m arrays]\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torchvision\\transforms\\functional.py:1118\u001b[0m, in \u001b[0;36mrotate\u001b[1;34m(img, angle, interpolation, expand, center, fill)\u001b[0m\n\u001b[0;32m   1115\u001b[0m \u001b[39m# due to current incoherence of rotation angle direction between affine and rotate implementations\u001b[39;00m\n\u001b[0;32m   1116\u001b[0m \u001b[39m# we need to set -angle.\u001b[39;00m\n\u001b[0;32m   1117\u001b[0m matrix \u001b[39m=\u001b[39m _get_inverse_affine_matrix(center_f, \u001b[39m-\u001b[39mangle, [\u001b[39m0.0\u001b[39m, \u001b[39m0.0\u001b[39m], \u001b[39m1.0\u001b[39m, [\u001b[39m0.0\u001b[39m, \u001b[39m0.0\u001b[39m])\n\u001b[1;32m-> 1118\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49mrotate(img, matrix\u001b[39m=\u001b[39;49mmatrix, interpolation\u001b[39m=\u001b[39;49minterpolation\u001b[39m.\u001b[39;49mvalue, expand\u001b[39m=\u001b[39;49mexpand, fill\u001b[39m=\u001b[39;49mfill)\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:671\u001b[0m, in \u001b[0;36mrotate\u001b[1;34m(img, matrix, interpolation, expand, fill)\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[39m# grid will be generated on the same device as theta and img\u001b[39;00m\n\u001b[0;32m    669\u001b[0m grid \u001b[39m=\u001b[39m _gen_affine_grid(theta, w\u001b[39m=\u001b[39mw, h\u001b[39m=\u001b[39mh, ow\u001b[39m=\u001b[39mow, oh\u001b[39m=\u001b[39moh)\n\u001b[1;32m--> 671\u001b[0m \u001b[39mreturn\u001b[39;00m _apply_grid_transform(img, grid, interpolation, fill\u001b[39m=\u001b[39;49mfill)\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:562\u001b[0m, in \u001b[0;36m_apply_grid_transform\u001b[1;34m(img, grid, mode, fill)\u001b[0m\n\u001b[0;32m    559\u001b[0m     mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones((img\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m, img\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m], img\u001b[39m.\u001b[39mshape[\u001b[39m3\u001b[39m]), dtype\u001b[39m=\u001b[39mimg\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39mimg\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    560\u001b[0m     img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((img, mask), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m--> 562\u001b[0m img \u001b[39m=\u001b[39m grid_sample(img, grid, mode\u001b[39m=\u001b[39;49mmode, padding_mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mzeros\u001b[39;49m\u001b[39m\"\u001b[39;49m, align_corners\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    564\u001b[0m \u001b[39m# Fill with required color\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[39mif\u001b[39;00m fill \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torch\\nn\\functional.py:4235\u001b[0m, in \u001b[0;36mgrid_sample\u001b[1;34m(input, grid, mode, padding_mode, align_corners)\u001b[0m\n\u001b[0;32m   4227\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   4228\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDefault grid_sample and affine_grid behavior has changed \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4229\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mto align_corners=False since 1.3.0. Please specify \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4230\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39malign_corners=True if the old behavior is desired. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4231\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee the documentation of grid_sample for details.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4232\u001b[0m     )\n\u001b[0;32m   4233\u001b[0m     align_corners \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 4235\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mgrid_sampler(\u001b[39minput\u001b[39;49m, grid, mode_enum, padding_mode_enum, align_corners)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Train Hamwood reference port in Python\n",
    "# import sys\n",
    "# sys.path.append(\"..\")\n",
    "\n",
    "# from components.hamwood import Hamwood\n",
    "# from components.datasets import ConesDataset\n",
    "\n",
    "# # Config\n",
    "# k = 8\n",
    "# epochs_per_fold = 25\n",
    "# dataset_name = \"20230301_cross_eroded_cropped\"\n",
    "\n",
    "# # Create dataset\n",
    "# dataset = ConesDataset(f\"F:/Data/ConeDatasets/{dataset_name}/train\", train=True, \n",
    "#     convert_to_dt=False,\n",
    "#     convert_to_gaussians=False,\n",
    "#     pre_generated=True, \n",
    "#     augment_rotate=True, \n",
    "#     augment_blur=False,\n",
    "#     augment_elastic=False,\n",
    "#     adaptive_dilation=False,\n",
    "#     dilation = 1) \n",
    "\n",
    "# # Train WITH all k folds\n",
    "# dataloaders = dataset.get_dataloaders_k_fold_cross_validation(k, batch_size=32)\n",
    "# Hamwood.train_networks_k_fold_cross_validation(dataloaders, max_epochs=epochs_per_fold, early_stopping=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train FN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\cones\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Anaconda3\\envs\\cones\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average running TRAINING loss for epoch 1: 0.4336323504304064\n",
      "Average running VALIDATION loss for epoch 1: 0.3319357826429255\n",
      "Average running TRAINING loss for epoch 2: 0.3109303093675909\n",
      "Average running VALIDATION loss for epoch 2: 0.28470051551566405\n",
      "Average running TRAINING loss for epoch 3: 0.27993377837641487\n",
      "Average running VALIDATION loss for epoch 3: 0.28295116038883433\n",
      "Average running TRAINING loss for epoch 4: 0.26226563314939366\n",
      "Average running VALIDATION loss for epoch 4: 0.2836194082218058\n",
      "Average running TRAINING loss for epoch 5: 0.24813257793671098\n",
      "Average running VALIDATION loss for epoch 5: 0.27203455655013814\n",
      "Average running TRAINING loss for epoch 6: 0.23341428083849364\n",
      "Average running VALIDATION loss for epoch 6: 0.2630419722374748\n",
      "Average running TRAINING loss for epoch 7: 0.22970670949796151\n",
      "Average running VALIDATION loss for epoch 7: 0.27344947001513314\n",
      "Average running TRAINING loss for epoch 8: 0.22336993051757073\n",
      "Average running VALIDATION loss for epoch 8: 0.25947944995234995\n",
      "Average running TRAINING loss for epoch 9: 0.218020789646383\n",
      "Average running VALIDATION loss for epoch 9: 0.2595856207258561\n",
      "Average running TRAINING loss for epoch 10: 0.21506169425516292\n",
      "Average running VALIDATION loss for epoch 10: 0.2628346909494961\n",
      "Average running TRAINING loss for epoch 11: 0.20742438932688073\n",
      "Average running VALIDATION loss for epoch 11: 0.2631482034921646\n",
      "Average running TRAINING loss for epoch 12: 0.20283636168159289\n",
      "Average running VALIDATION loss for epoch 12: 0.2416799559312708\n",
      "Average running TRAINING loss for epoch 13: 0.20485086925327778\n",
      "Average running VALIDATION loss for epoch 13: 0.2491773463347379\n",
      "Average running TRAINING loss for epoch 14: 0.1998183397126609\n",
      "Average running VALIDATION loss for epoch 14: 0.24696276293081396\n",
      "Average running TRAINING loss for epoch 15: 0.1921327755754364\n",
      "Average running VALIDATION loss for epoch 15: 0.2567206507219988\n",
      "Average running TRAINING loss for epoch 16: 0.19006017123445354\n",
      "Average running VALIDATION loss for epoch 16: 0.253884159905069\n",
      "Average running TRAINING loss for epoch 17: 0.18904223620634655\n",
      "Average running VALIDATION loss for epoch 17: 0.25341913410845923\n",
      "Average running TRAINING loss for epoch 18: 0.1883454625655351\n",
      "Average running VALIDATION loss for epoch 18: 0.24543243997237263\n",
      "Average running TRAINING loss for epoch 19: 0.18294931716960053\n",
      "Average running VALIDATION loss for epoch 19: 0.23836726826779983\n",
      "Average running TRAINING loss for epoch 20: 0.17845976789449824\n",
      "Average running VALIDATION loss for epoch 20: 0.26085078979239745\n",
      "Average running TRAINING loss for epoch 21: 0.18433266149127278\n",
      "Average running VALIDATION loss for epoch 21: 0.2317523255067713\n",
      "Average running TRAINING loss for epoch 22: 0.17531967265852566\n",
      "Average running VALIDATION loss for epoch 22: 0.23598581289543824\n",
      "Average running TRAINING loss for epoch 23: 0.17493091562184795\n",
      "Average running VALIDATION loss for epoch 23: 0.2502781266675276\n",
      "Average running TRAINING loss for epoch 24: 0.1731902450836938\n",
      "Average running VALIDATION loss for epoch 24: 0.26285694714854746\n",
      "Average running TRAINING loss for epoch 25: 0.17104671828063397\n",
      "Average running VALIDATION loss for epoch 25: 0.25787096076151905\n",
      "Average running TRAINING loss for epoch 26: 0.1695810726940118\n",
      "Average running VALIDATION loss for epoch 26: 0.2649962849476758\n",
      "Average running TRAINING loss for epoch 27: 0.16701084930963558\n",
      "Average running VALIDATION loss for epoch 27: 0.25464563860612754\n",
      "Average running TRAINING loss for epoch 28: 0.1618766261842744\n",
      "Average running VALIDATION loss for epoch 28: 0.24155264829888062\n",
      "Average running TRAINING loss for epoch 29: 0.16528939860391206\n",
      "Average running VALIDATION loss for epoch 29: 0.24387402306584752\n",
      "Average running TRAINING loss for epoch 30: 0.16174734814156747\n",
      "Average running VALIDATION loss for epoch 30: 0.24763298867379918\n",
      "Average running TRAINING loss for epoch 31: 0.1604493827848085\n",
      "Average running VALIDATION loss for epoch 31: 0.25008897571002736\n",
      "Average running TRAINING loss for epoch 32: 0.15310392143397494\n",
      "Average running VALIDATION loss for epoch 32: 0.24324039676610162\n",
      "Average running TRAINING loss for epoch 33: 0.1616160966263249\n",
      "Average running VALIDATION loss for epoch 33: 0.26139506434693055\n",
      "Average running TRAINING loss for epoch 34: 0.15408556951723737\n",
      "Average running VALIDATION loss for epoch 34: 0.27344296346692476\n",
      "Average running TRAINING loss for epoch 35: 0.16118310029008265\n",
      "Average running VALIDATION loss for epoch 35: 0.2659093965502346\n",
      "Average running TRAINING loss for epoch 36: 0.15320745258238808\n",
      "Average running VALIDATION loss for epoch 36: 0.2620050271644312\n",
      "Average running TRAINING loss for epoch 37: 0.15752288811551085\n",
      "Average running VALIDATION loss for epoch 37: 0.25433700356413336\n",
      "Average running TRAINING loss for epoch 38: 0.1475906788917451\n",
      "Average running VALIDATION loss for epoch 38: 0.26591744536862655\n",
      "Average running TRAINING loss for epoch 39: 0.15082601611984187\n",
      "Average running VALIDATION loss for epoch 39: 0.2657946313128752\n",
      "Average running TRAINING loss for epoch 40: 0.14411941520741273\n",
      "Average running VALIDATION loss for epoch 40: 0.2819386015043539\n",
      "Fold 0: 40 epochs, (0.14411941520741273,0.2317523255067713)\n",
      "Average running TRAINING loss for epoch 1: 0.43431120211708135\n",
      "Average running VALIDATION loss for epoch 1: 0.3719512890366947\n",
      "Average running TRAINING loss for epoch 2: 0.306601499046745\n",
      "Average running VALIDATION loss for epoch 2: 0.35261014805120583\n",
      "Average running TRAINING loss for epoch 3: 0.28517536150998085\n",
      "Average running VALIDATION loss for epoch 3: 0.31250031818361845\n",
      "Average running TRAINING loss for epoch 4: 0.2622637690912033\n",
      "Average running VALIDATION loss for epoch 4: 0.2829975424443974\n",
      "Average running TRAINING loss for epoch 5: 0.24895750737652697\n",
      "Average running VALIDATION loss for epoch 5: 0.2684976958176669\n",
      "Average running TRAINING loss for epoch 6: 0.24307175815619272\n",
      "Average running VALIDATION loss for epoch 6: 0.2959318204837687\n",
      "Average running TRAINING loss for epoch 7: 0.2388092827951086\n",
      "Average running VALIDATION loss for epoch 7: 0.2728414355831988\n",
      "Average running TRAINING loss for epoch 8: 0.22876129389323038\n",
      "Average running VALIDATION loss for epoch 8: 0.2906645904569065\n",
      "Average running TRAINING loss for epoch 9: 0.22870990935841512\n",
      "Average running VALIDATION loss for epoch 9: 0.26443994746488686\n",
      "Average running TRAINING loss for epoch 10: 0.21846747128614064\n",
      "Average running VALIDATION loss for epoch 10: 0.276788906139486\n",
      "Average running TRAINING loss for epoch 11: 0.21289407738066954\n",
      "Average running VALIDATION loss for epoch 11: 0.2808324344017926\n",
      "Average running TRAINING loss for epoch 12: 0.2139652396455921\n",
      "Average running VALIDATION loss for epoch 12: 0.2643222055014442\n",
      "Average running TRAINING loss for epoch 13: 0.20849876732019515\n",
      "Average running VALIDATION loss for epoch 13: 0.2747473436243394\n",
      "Average running TRAINING loss for epoch 14: 0.20520581060956264\n",
      "Average running VALIDATION loss for epoch 14: 0.2725020419148838\n",
      "Average running TRAINING loss for epoch 15: 0.19954627077898074\n",
      "Average running VALIDATION loss for epoch 15: 0.24502570576527538\n",
      "Average running TRAINING loss for epoch 16: 0.1983120926045652\n",
      "Average running VALIDATION loss for epoch 16: 0.2643554017824285\n",
      "Average running TRAINING loss for epoch 17: 0.19127741584490085\n",
      "Average running VALIDATION loss for epoch 17: 0.27036356728743105\n",
      "Average running TRAINING loss for epoch 18: 0.1923185889448585\n",
      "Average running VALIDATION loss for epoch 18: 0.26059193909168243\n",
      "Average running TRAINING loss for epoch 19: 0.19489258386451622\n",
      "Average running VALIDATION loss for epoch 19: 0.25376055258161884\n",
      "Average running TRAINING loss for epoch 20: 0.18290814506853448\n",
      "Average running VALIDATION loss for epoch 20: 0.25895841945620146\n",
      "Average running TRAINING loss for epoch 21: 0.18442380987107754\n",
      "Average running VALIDATION loss for epoch 21: 0.256546032779357\n",
      "Average running TRAINING loss for epoch 22: 0.1829244213756816\n",
      "Average running VALIDATION loss for epoch 22: 0.2797490936868331\n",
      "Average running TRAINING loss for epoch 23: 0.18105363068652564\n",
      "Average running VALIDATION loss for epoch 23: 0.2846049762824002\n",
      "Average running TRAINING loss for epoch 24: 0.17639038885204955\n",
      "Average running VALIDATION loss for epoch 24: 0.2661044089233174\n",
      "Average running TRAINING loss for epoch 25: 0.18755160548306746\n",
      "Average running VALIDATION loss for epoch 25: 0.26381483849357157\n",
      "Average running TRAINING loss for epoch 26: 0.16733900348431077\n",
      "Average running VALIDATION loss for epoch 26: 0.2760168178116574\n",
      "Average running TRAINING loss for epoch 27: 0.17540623774302416\n",
      "Average running VALIDATION loss for epoch 27: 0.25855545874904184\n",
      "Average running TRAINING loss for epoch 28: 0.1703454464160163\n",
      "Average running VALIDATION loss for epoch 28: 0.25958630092003765\n",
      "Average running TRAINING loss for epoch 29: 0.1685561690929121\n",
      "Average running VALIDATION loss for epoch 29: 0.2736086705151726\n",
      "Average running TRAINING loss for epoch 30: 0.16803001580310278\n",
      "Average running VALIDATION loss for epoch 30: 0.2942413398448159\n",
      "Fold 1: 30 epochs, (0.16733900348431077,0.24502570576527538)\n",
      "Average running TRAINING loss for epoch 1: 0.4433032687881897\n",
      "Average running VALIDATION loss for epoch 1: 0.32093138203901406\n",
      "Average running TRAINING loss for epoch 2: 0.32048689496928245\n",
      "Average running VALIDATION loss for epoch 2: 0.30504421276204724\n",
      "Average running TRAINING loss for epoch 3: 0.28347003909534424\n",
      "Average running VALIDATION loss for epoch 3: 0.2750149404301363\n",
      "Average running TRAINING loss for epoch 4: 0.267614772029478\n",
      "Average running VALIDATION loss for epoch 4: 0.2721818939727895\n",
      "Average running TRAINING loss for epoch 5: 0.25278444228501157\n",
      "Average running VALIDATION loss for epoch 5: 0.26980046139043923\n",
      "Average running TRAINING loss for epoch 6: 0.2474429852746684\n",
      "Average running VALIDATION loss for epoch 6: 0.26841913777239185\n",
      "Average running TRAINING loss for epoch 7: 0.23028147734444718\n",
      "Average running VALIDATION loss for epoch 7: 0.2363634574062684\n",
      "Average running TRAINING loss for epoch 8: 0.2293346936589685\n",
      "Average running VALIDATION loss for epoch 8: 0.26197704497505636\n",
      "Average running TRAINING loss for epoch 9: 0.216715421912999\n",
      "Average running VALIDATION loss for epoch 9: 0.242679060820271\n",
      "Average running TRAINING loss for epoch 10: 0.21811119023838949\n",
      "Average running VALIDATION loss for epoch 10: 0.23960172341150396\n",
      "Average running TRAINING loss for epoch 11: 0.2095537850761722\n",
      "Average running VALIDATION loss for epoch 11: 0.24469862264745376\n",
      "Average running TRAINING loss for epoch 12: 0.20082910773183765\n",
      "Average running VALIDATION loss for epoch 12: 0.2301157634047901\n",
      "Average running TRAINING loss for epoch 13: 0.20394503222457294\n",
      "Average running VALIDATION loss for epoch 13: 0.2511887138380724\n",
      "Average running TRAINING loss for epoch 14: 0.20531488139310788\n",
      "Average running VALIDATION loss for epoch 14: 0.2221150385106311\n",
      "Average running TRAINING loss for epoch 15: 0.19313744860219545\n",
      "Average running VALIDATION loss for epoch 15: 0.23824511380756602\n",
      "Average running TRAINING loss for epoch 16: 0.18839165639003802\n",
      "Average running VALIDATION loss for epoch 16: 0.24062372831737294\n",
      "Average running TRAINING loss for epoch 17: 0.19109488728231397\n",
      "Average running VALIDATION loss for epoch 17: 0.26569481632288766\n",
      "Average running TRAINING loss for epoch 18: 0.1911771914696899\n",
      "Average running VALIDATION loss for epoch 18: 0.23070859821403727\n",
      "Average running TRAINING loss for epoch 19: 0.1829552735234129\n",
      "Average running VALIDATION loss for epoch 19: 0.2306172409478356\n",
      "Average running TRAINING loss for epoch 20: 0.18688108116902155\n",
      "Average running VALIDATION loss for epoch 20: 0.23109214533777797\n",
      "Average running TRAINING loss for epoch 21: 0.18006675516993836\n",
      "Average running VALIDATION loss for epoch 21: 0.25584418195135455\n",
      "Average running TRAINING loss for epoch 22: 0.1773792913761632\n",
      "Average running VALIDATION loss for epoch 22: 0.24778193586012898\n",
      "Average running TRAINING loss for epoch 23: 0.17456031121827406\n",
      "Average running VALIDATION loss for epoch 23: 0.23374308207455805\n",
      "Average running TRAINING loss for epoch 24: 0.1754933681082109\n",
      "Average running VALIDATION loss for epoch 24: 0.24708630845827215\n",
      "Average running TRAINING loss for epoch 25: 0.1711175320348863\n",
      "Average running VALIDATION loss for epoch 25: 0.2613517624490401\n",
      "Average running TRAINING loss for epoch 26: 0.17034398340459528\n",
      "Average running VALIDATION loss for epoch 26: 0.2568998538395938\n",
      "Average running TRAINING loss for epoch 27: 0.1650137322922719\n",
      "Average running VALIDATION loss for epoch 27: 0.24821432636064641\n",
      "Average running TRAINING loss for epoch 28: 0.16625303033225494\n",
      "Average running VALIDATION loss for epoch 28: 0.25791265859323387\n",
      "Average running TRAINING loss for epoch 29: 0.16673787619019376\n",
      "Average running VALIDATION loss for epoch 29: 0.2431180766400169\n",
      "Average running TRAINING loss for epoch 30: 0.16532795699634428\n",
      "Average running VALIDATION loss for epoch 30: 0.24877346613827875\n",
      "Average running TRAINING loss for epoch 31: 0.15799004690528945\n",
      "Average running VALIDATION loss for epoch 31: 0.24469559332903693\n",
      "Average running TRAINING loss for epoch 32: 0.15793103442109865\n",
      "Average running VALIDATION loss for epoch 32: 0.2522985702928375\n",
      "Average running TRAINING loss for epoch 33: 0.16204802215035105\n",
      "Average running VALIDATION loss for epoch 33: 0.26015136697713065\n",
      "Average running TRAINING loss for epoch 34: 0.15773110509175678\n",
      "Average running VALIDATION loss for epoch 34: 0.26056506703881654\n",
      "Average running TRAINING loss for epoch 35: 0.15562247639072352\n",
      "Average running VALIDATION loss for epoch 35: 0.2537744518588571\n",
      "Average running TRAINING loss for epoch 36: 0.15886424854397774\n",
      "Average running VALIDATION loss for epoch 36: 0.26528778409256654\n",
      "Average running TRAINING loss for epoch 37: 0.155516366056841\n",
      "Average running VALIDATION loss for epoch 37: 0.2529568663414787\n",
      "Average running TRAINING loss for epoch 38: 0.1529758925838717\n",
      "Average running VALIDATION loss for epoch 38: 0.26049794256687164\n",
      "Average running TRAINING loss for epoch 39: 0.15483356941619825\n",
      "Average running VALIDATION loss for epoch 39: 0.24006289769621456\n",
      "Average running TRAINING loss for epoch 40: 0.1509061473068492\n",
      "Average running VALIDATION loss for epoch 40: 0.25347312934258404\n",
      "Average running TRAINING loss for epoch 41: 0.14572064523938402\n",
      "Average running VALIDATION loss for epoch 41: 0.29353368457625895\n",
      "Fold 2: 41 epochs, (0.14572064523938402,0.2221150385106311)\n",
      "Average running TRAINING loss for epoch 1: 0.4379325457688036\n",
      "Average running VALIDATION loss for epoch 1: 0.35399680014918833\n",
      "Average running TRAINING loss for epoch 2: 0.29477337516587354\n",
      "Average running VALIDATION loss for epoch 2: 0.3172328743864508\n",
      "Average running TRAINING loss for epoch 3: 0.26309662357229613\n",
      "Average running VALIDATION loss for epoch 3: 0.3064086007721284\n",
      "Average running TRAINING loss for epoch 4: 0.2491666166561431\n",
      "Average running VALIDATION loss for epoch 4: 0.2869507819414139\n",
      "Average running TRAINING loss for epoch 5: 0.23915265744616246\n",
      "Average running VALIDATION loss for epoch 5: 0.2633618651067509\n",
      "Average running TRAINING loss for epoch 6: 0.22707358099006372\n",
      "Average running VALIDATION loss for epoch 6: 0.3013846225598279\n",
      "Average running TRAINING loss for epoch 7: 0.2241048513558404\n",
      "Average running VALIDATION loss for epoch 7: 0.27280160872375264\n",
      "Average running TRAINING loss for epoch 8: 0.21261004576909132\n",
      "Average running VALIDATION loss for epoch 8: 0.28359661295133476\n",
      "Average running TRAINING loss for epoch 9: 0.2073601447045803\n",
      "Average running VALIDATION loss for epoch 9: 0.25843977971988563\n",
      "Average running TRAINING loss for epoch 10: 0.20532963810295896\n",
      "Average running VALIDATION loss for epoch 10: 0.2573081991251777\n",
      "Average running TRAINING loss for epoch 11: 0.20254058519314075\n",
      "Average running VALIDATION loss for epoch 11: 0.26953572823720817\n",
      "Average running TRAINING loss for epoch 12: 0.19394480700379815\n",
      "Average running VALIDATION loss for epoch 12: 0.2562267144813257\n",
      "Average running TRAINING loss for epoch 13: 0.19546317277026587\n",
      "Average running VALIDATION loss for epoch 13: 0.25835976662004695\n",
      "Average running TRAINING loss for epoch 14: 0.18812856521328974\n",
      "Average running VALIDATION loss for epoch 14: 0.26139106338515\n",
      "Average running TRAINING loss for epoch 15: 0.18784459269252315\n",
      "Average running VALIDATION loss for epoch 15: 0.2675626014961916\n",
      "Average running TRAINING loss for epoch 16: 0.18527024733866082\n",
      "Average running VALIDATION loss for epoch 16: 0.2681117066565682\n",
      "Average running TRAINING loss for epoch 17: 0.1800041613650733\n",
      "Average running VALIDATION loss for epoch 17: 0.26413603302310495\n",
      "Average running TRAINING loss for epoch 18: 0.17768693252884107\n",
      "Average running VALIDATION loss for epoch 18: 0.256009586593684\n",
      "Average running TRAINING loss for epoch 19: 0.17159689221017319\n",
      "Average running VALIDATION loss for epoch 19: 0.249418705701828\n",
      "Average running TRAINING loss for epoch 20: 0.16659184079617262\n",
      "Average running VALIDATION loss for epoch 20: 0.2698936295859954\n",
      "Average running TRAINING loss for epoch 21: 0.16508985593401151\n",
      "Average running VALIDATION loss for epoch 21: 0.2771005385062274\n",
      "Average running TRAINING loss for epoch 22: 0.16978409722575855\n",
      "Average running VALIDATION loss for epoch 22: 0.2823300747310414\n",
      "Average running TRAINING loss for epoch 23: 0.1580303654973877\n",
      "Average running VALIDATION loss for epoch 23: 0.26827164432581735\n",
      "Average running TRAINING loss for epoch 24: 0.15892750606069278\n",
      "Average running VALIDATION loss for epoch 24: 0.26863491710494547\n",
      "Average running TRAINING loss for epoch 25: 0.1557119469126237\n",
      "Average running VALIDATION loss for epoch 25: 0.2602106841171489\n",
      "Average running TRAINING loss for epoch 26: 0.1549507950850088\n",
      "Average running VALIDATION loss for epoch 26: 0.24673815991948633\n",
      "Average running TRAINING loss for epoch 27: 0.15732534743588547\n",
      "Average running VALIDATION loss for epoch 27: 0.28088846890365377\n",
      "Average running TRAINING loss for epoch 28: 0.15381970352910715\n",
      "Average running VALIDATION loss for epoch 28: 0.2748989791554563\n",
      "Average running TRAINING loss for epoch 29: 0.14849700720916534\n",
      "Average running VALIDATION loss for epoch 29: 0.2854026485891903\n",
      "Average running TRAINING loss for epoch 30: 0.15144428694299583\n",
      "Average running VALIDATION loss for epoch 30: 0.27911294646122875\n",
      "Average running TRAINING loss for epoch 31: 0.14660883116824874\n",
      "Average running VALIDATION loss for epoch 31: 0.2722110064590679\n",
      "Average running TRAINING loss for epoch 32: 0.14654222069757766\n",
      "Average running VALIDATION loss for epoch 32: 0.2673666819053538\n",
      "Average running TRAINING loss for epoch 33: 0.14547387702422665\n",
      "Average running VALIDATION loss for epoch 33: 0.2625947826925446\n",
      "Average running TRAINING loss for epoch 34: 0.13740278340490727\n",
      "Average running VALIDATION loss for epoch 34: 0.2829008417970994\n",
      "Average running TRAINING loss for epoch 35: 0.14048954629307164\n",
      "Average running VALIDATION loss for epoch 35: 0.28384513451772575\n",
      "Average running TRAINING loss for epoch 36: 0.1417456972573338\n",
      "Average running VALIDATION loss for epoch 36: 0.2817995013559566\n",
      "Average running TRAINING loss for epoch 37: 0.13984840077444397\n",
      "Average running VALIDATION loss for epoch 37: 0.291858187493156\n",
      "Average running TRAINING loss for epoch 38: 0.1354629792080357\n",
      "Average running VALIDATION loss for epoch 38: 0.2816987484693527\n",
      "Average running TRAINING loss for epoch 39: 0.13414429995263444\n",
      "Average running VALIDATION loss for epoch 39: 0.2818794419222018\n",
      "Average running TRAINING loss for epoch 40: 0.13831985377353326\n",
      "Average running VALIDATION loss for epoch 40: 0.2819432405864491\n",
      "Average running TRAINING loss for epoch 41: 0.1351812305892336\n",
      "Average running VALIDATION loss for epoch 41: 0.3138821633423076\n",
      "Fold 3: 41 epochs, (0.13414429995263444,0.24673815991948633)\n",
      "Average running TRAINING loss for epoch 1: 0.4497499170488325\n",
      "Average running VALIDATION loss for epoch 1: 0.32373791319482464\n",
      "Average running TRAINING loss for epoch 2: 0.31962931502995817\n",
      "Average running VALIDATION loss for epoch 2: 0.3164006226203021\n",
      "Average running TRAINING loss for epoch 3: 0.29633702479045965\n",
      "Average running VALIDATION loss for epoch 3: 0.27842284563709707\n",
      "Average running TRAINING loss for epoch 4: 0.2701956133904128\n",
      "Average running VALIDATION loss for epoch 4: 0.2687843704924864\n",
      "Average running TRAINING loss for epoch 5: 0.2654550227111784\n",
      "Average running VALIDATION loss for epoch 5: 0.24800138832891688\n",
      "Average running TRAINING loss for epoch 6: 0.2513618811579614\n",
      "Average running VALIDATION loss for epoch 6: 0.2645353262915331\n",
      "Average running TRAINING loss for epoch 7: 0.24289451134872847\n",
      "Average running VALIDATION loss for epoch 7: 0.25229215665775184\n",
      "Average running TRAINING loss for epoch 8: 0.23633237093173223\n",
      "Average running VALIDATION loss for epoch 8: 0.2639469723491108\n",
      "Average running TRAINING loss for epoch 9: 0.23011146389461798\n",
      "Average running VALIDATION loss for epoch 9: 0.23822384196169236\n",
      "Average running TRAINING loss for epoch 10: 0.22071811261362043\n",
      "Average running VALIDATION loss for epoch 10: 0.24812595370937796\n",
      "Average running TRAINING loss for epoch 11: 0.21862033105872827\n",
      "Average running VALIDATION loss for epoch 11: 0.24435705866883783\n",
      "Average running TRAINING loss for epoch 12: 0.2191006800095583\n",
      "Average running VALIDATION loss for epoch 12: 0.2518492744249456\n",
      "Average running TRAINING loss for epoch 13: 0.2174628622316081\n",
      "Average running VALIDATION loss for epoch 13: 0.2537301883978002\n",
      "Average running TRAINING loss for epoch 14: 0.2035338349383453\n",
      "Average running VALIDATION loss for epoch 14: 0.24712214575094335\n",
      "Average running TRAINING loss for epoch 15: 0.20535196071683331\n",
      "Average running VALIDATION loss for epoch 15: 0.23952103625325596\n",
      "Average running TRAINING loss for epoch 16: 0.20463684169125967\n",
      "Average running VALIDATION loss for epoch 16: 0.2267942818648675\n",
      "Average running TRAINING loss for epoch 17: 0.19578688026502214\n",
      "Average running VALIDATION loss for epoch 17: 0.22796603073092067\n",
      "Average running TRAINING loss for epoch 18: 0.18998320734706417\n",
      "Average running VALIDATION loss for epoch 18: 0.25226916591910753\n",
      "Average running TRAINING loss for epoch 19: 0.19610198484412555\n",
      "Average running VALIDATION loss for epoch 19: 0.2419239755939035\n",
      "Average running TRAINING loss for epoch 20: 0.18862636547921033\n",
      "Average running VALIDATION loss for epoch 20: 0.24495811497463899\n",
      "Average running TRAINING loss for epoch 21: 0.18862923157626185\n",
      "Average running VALIDATION loss for epoch 21: 0.23199844623313232\n",
      "Average running TRAINING loss for epoch 22: 0.18587127366456493\n",
      "Average running VALIDATION loss for epoch 22: 0.22534330978113062\n",
      "Average running TRAINING loss for epoch 23: 0.18009383385551386\n",
      "Average running VALIDATION loss for epoch 23: 0.2472677182625322\n",
      "Average running TRAINING loss for epoch 24: 0.18025992560232507\n",
      "Average running VALIDATION loss for epoch 24: 0.23566172447274714\n",
      "Average running TRAINING loss for epoch 25: 0.1766144570990883\n",
      "Average running VALIDATION loss for epoch 25: 0.2264649263199638\n",
      "Average running TRAINING loss for epoch 26: 0.17546597421811572\n",
      "Average running VALIDATION loss for epoch 26: 0.24371561714831522\n",
      "Average running TRAINING loss for epoch 27: 0.1733737899440116\n",
      "Average running VALIDATION loss for epoch 27: 0.23496619831113255\n",
      "Average running TRAINING loss for epoch 28: 0.1752397926716969\n",
      "Average running VALIDATION loss for epoch 28: 0.23561110654297998\n",
      "Average running TRAINING loss for epoch 29: 0.16393118608614493\n",
      "Average running VALIDATION loss for epoch 29: 0.22702795968336217\n",
      "Average running TRAINING loss for epoch 30: 0.1632251058030745\n",
      "Average running VALIDATION loss for epoch 30: 0.24881987186039195\n",
      "Average running TRAINING loss for epoch 31: 0.16678802360748424\n",
      "Average running VALIDATION loss for epoch 31: 0.23518267010941224\n",
      "Average running TRAINING loss for epoch 32: 0.1598311935005517\n",
      "Average running VALIDATION loss for epoch 32: 0.24397879838943481\n",
      "Average running TRAINING loss for epoch 33: 0.16683362706981855\n",
      "Average running VALIDATION loss for epoch 33: 0.26382530875065746\n",
      "Average running TRAINING loss for epoch 34: 0.15803286532775082\n",
      "Average running VALIDATION loss for epoch 34: 0.2581728688057731\n",
      "Average running TRAINING loss for epoch 35: 0.1631499132591075\n",
      "Average running VALIDATION loss for epoch 35: 0.24450604749076507\n",
      "Average running TRAINING loss for epoch 36: 0.1544708980504295\n",
      "Average running VALIDATION loss for epoch 36: 0.24595216091941385\n",
      "Average running TRAINING loss for epoch 37: 0.15916183256897434\n",
      "Average running VALIDATION loss for epoch 37: 0.24042661707190907\n",
      "Average running TRAINING loss for epoch 38: 0.15457131771434998\n",
      "Average running VALIDATION loss for epoch 38: 0.24576305159751108\n",
      "Average running TRAINING loss for epoch 39: 0.15482085615653415\n",
      "Average running VALIDATION loss for epoch 39: 0.2491537424571374\n",
      "Average running TRAINING loss for epoch 40: 0.1521096926439425\n",
      "Average running VALIDATION loss for epoch 40: 0.25208160105873556\n",
      "Average running TRAINING loss for epoch 41: 0.1492324922683424\n",
      "Average running VALIDATION loss for epoch 41: 0.2601098698728225\n",
      "Average running TRAINING loss for epoch 42: 0.1549404718238732\n",
      "Average running VALIDATION loss for epoch 42: 0.2521536508027245\n",
      "Average running TRAINING loss for epoch 43: 0.14845827420980767\n",
      "Average running VALIDATION loss for epoch 43: 0.24226260974126704\n",
      "Average running TRAINING loss for epoch 44: 0.14656668933558054\n",
      "Average running VALIDATION loss for epoch 44: 0.2433882557293948\n",
      "Average running TRAINING loss for epoch 45: 0.1465230076734362\n",
      "Average running VALIDATION loss for epoch 45: 0.2541547443936853\n",
      "Average running TRAINING loss for epoch 46: 0.14269305043050956\n",
      "Average running VALIDATION loss for epoch 46: 0.27702078749151793\n",
      "Fold 4: 46 epochs, (0.14269305043050956,0.22534330978113062)\n",
      "Average running TRAINING loss for epoch 1: 0.4445836266566967\n",
      "Average running VALIDATION loss for epoch 1: 0.33396054979632883\n",
      "Average running TRAINING loss for epoch 2: 0.3299868431841505\n",
      "Average running VALIDATION loss for epoch 2: 0.3180675138445461\n",
      "Average running TRAINING loss for epoch 3: 0.29769263570678645\n",
      "Average running VALIDATION loss for epoch 3: 0.28139110961381125\n",
      "Average running TRAINING loss for epoch 4: 0.27742293425675096\n",
      "Average running VALIDATION loss for epoch 4: 0.2615155148155549\n",
      "Average running TRAINING loss for epoch 5: 0.26427586871231423\n",
      "Average running VALIDATION loss for epoch 5: 0.2550189302248113\n",
      "Average running TRAINING loss for epoch 6: 0.24568194028889312\n",
      "Average running VALIDATION loss for epoch 6: 0.2515179225627114\n",
      "Average running TRAINING loss for epoch 7: 0.24152247710474606\n",
      "Average running VALIDATION loss for epoch 7: 0.2618393687640919\n",
      "Average running TRAINING loss for epoch 8: 0.2338170576840639\n",
      "Average running VALIDATION loss for epoch 8: 0.2511150407440522\n",
      "Average running TRAINING loss for epoch 9: 0.23092174902558327\n",
      "Average running VALIDATION loss for epoch 9: 0.22597761978121364\n",
      "Average running TRAINING loss for epoch 10: 0.2253731294567215\n",
      "Average running VALIDATION loss for epoch 10: 0.22752991932279923\n",
      "Average running TRAINING loss for epoch 11: 0.22009056324845758\n",
      "Average running VALIDATION loss for epoch 11: 0.23486654197468476\n",
      "Average running TRAINING loss for epoch 12: 0.21214766100305935\n",
      "Average running VALIDATION loss for epoch 12: 0.2429604766999974\n",
      "Average running TRAINING loss for epoch 13: 0.2157262472500061\n",
      "Average running VALIDATION loss for epoch 13: 0.22318975452114553\n",
      "Average running TRAINING loss for epoch 14: 0.20496030029808654\n",
      "Average running VALIDATION loss for epoch 14: 0.23061411170398488\n",
      "Average running TRAINING loss for epoch 15: 0.20705484316266817\n",
      "Average running VALIDATION loss for epoch 15: 0.21546242999679902\n",
      "Average running TRAINING loss for epoch 16: 0.20660500877119345\n",
      "Average running VALIDATION loss for epoch 16: 0.21514450802522547\n",
      "Average running TRAINING loss for epoch 17: 0.20034107122698736\n",
      "Average running VALIDATION loss for epoch 17: 0.22988329652477713\n",
      "Average running TRAINING loss for epoch 18: 0.1899887539510583\n",
      "Average running VALIDATION loss for epoch 18: 0.2053671435398214\n",
      "Average running TRAINING loss for epoch 19: 0.19384481552346\n",
      "Average running VALIDATION loss for epoch 19: 0.22486462926163392\n",
      "Average running TRAINING loss for epoch 20: 0.18921077514773813\n",
      "Average running VALIDATION loss for epoch 20: 0.22108528982190526\n",
      "Average running TRAINING loss for epoch 21: 0.18621801411540345\n",
      "Average running VALIDATION loss for epoch 21: 0.22768819682738362\n",
      "Average running TRAINING loss for epoch 22: 0.19022234298031906\n",
      "Average running VALIDATION loss for epoch 22: 0.20693139556576223\n",
      "Average running TRAINING loss for epoch 23: 0.18883675767173028\n",
      "Average running VALIDATION loss for epoch 23: 0.21504324586952434\n",
      "Average running TRAINING loss for epoch 24: 0.18289099161609493\n",
      "Average running VALIDATION loss for epoch 24: 0.21872134024606033\n",
      "Average running TRAINING loss for epoch 25: 0.18056719718051367\n",
      "Average running VALIDATION loss for epoch 25: 0.2368385151905172\n",
      "Average running TRAINING loss for epoch 26: 0.17678699533230272\n",
      "Average running VALIDATION loss for epoch 26: 0.21914576125495575\n",
      "Average running TRAINING loss for epoch 27: 0.17587504741446724\n",
      "Average running VALIDATION loss for epoch 27: 0.21533388688283808\n",
      "Average running TRAINING loss for epoch 28: 0.1769777020632193\n",
      "Average running VALIDATION loss for epoch 28: 0.22064662154983072\n",
      "Average running TRAINING loss for epoch 29: 0.1714476590237484\n",
      "Average running VALIDATION loss for epoch 29: 0.22236672043800354\n",
      "Average running TRAINING loss for epoch 30: 0.1707268779904678\n",
      "Average running VALIDATION loss for epoch 30: 0.22104432433843613\n",
      "Average running TRAINING loss for epoch 31: 0.16802656701926527\n",
      "Average running VALIDATION loss for epoch 31: 0.23995482220369227\n",
      "Average running TRAINING loss for epoch 32: 0.16886550021068802\n",
      "Average running VALIDATION loss for epoch 32: 0.2203579935080865\n",
      "Average running TRAINING loss for epoch 33: 0.168984095919235\n",
      "Average running VALIDATION loss for epoch 33: 0.2283614812528386\n",
      "Average running TRAINING loss for epoch 34: 0.1648314848285297\n",
      "Average running VALIDATION loss for epoch 34: 0.24201930708744945\n",
      "Average running TRAINING loss for epoch 35: 0.16408579857570343\n",
      "Average running VALIDATION loss for epoch 35: 0.20859251609619925\n",
      "Average running TRAINING loss for epoch 36: 0.1641771788227147\n",
      "Average running VALIDATION loss for epoch 36: 0.22885829867685542\n",
      "Average running TRAINING loss for epoch 37: 0.16193264655383496\n",
      "Average running VALIDATION loss for epoch 37: 0.21455052100560246\n",
      "Average running TRAINING loss for epoch 38: 0.1560942064649586\n",
      "Average running VALIDATION loss for epoch 38: 0.23972721660838409\n",
      "Average running TRAINING loss for epoch 39: 0.16407344008571115\n",
      "Average running VALIDATION loss for epoch 39: 0.2365672759273473\n",
      "Average running TRAINING loss for epoch 40: 0.15677872219861583\n",
      "Average running VALIDATION loss for epoch 40: 0.24013749071780374\n",
      "Average running TRAINING loss for epoch 41: 0.15583186204834232\n",
      "Average running VALIDATION loss for epoch 41: 0.2243217644007767\n",
      "Average running TRAINING loss for epoch 42: 0.15970792595682473\n",
      "Average running VALIDATION loss for epoch 42: 0.23825493542587056\n",
      "Average running TRAINING loss for epoch 43: 0.15291069669584775\n",
      "Average running VALIDATION loss for epoch 43: 0.22820796050569592\n",
      "Average running TRAINING loss for epoch 44: 0.15256424360619536\n",
      "Average running VALIDATION loss for epoch 44: 0.2139875468085794\n",
      "Average running TRAINING loss for epoch 45: 0.15266943777558104\n",
      "Average running VALIDATION loss for epoch 45: 0.23176823731730967\n",
      "Average running TRAINING loss for epoch 46: 0.15345304407950106\n",
      "Average running VALIDATION loss for epoch 46: 0.250401653787669\n",
      "Fold 5: 46 epochs, (0.15256424360619536,0.2053671435398214)\n",
      "Average running TRAINING loss for epoch 1: 0.417585963832921\n",
      "Average running VALIDATION loss for epoch 1: 0.3209031592397129\n",
      "Average running TRAINING loss for epoch 2: 0.31272374530290736\n",
      "Average running VALIDATION loss for epoch 2: 0.28075167010812196\n",
      "Average running TRAINING loss for epoch 3: 0.28668375839960986\n",
      "Average running VALIDATION loss for epoch 3: 0.27082324378630696\n",
      "Average running TRAINING loss for epoch 4: 0.2633820069504195\n",
      "Average running VALIDATION loss for epoch 4: 0.26915542693699107\n",
      "Average running TRAINING loss for epoch 5: 0.24396830443935147\n",
      "Average running VALIDATION loss for epoch 5: 0.2579886054291445\n",
      "Average running TRAINING loss for epoch 6: 0.2421822025087373\n",
      "Average running VALIDATION loss for epoch 6: 0.2485308068640092\n",
      "Average running TRAINING loss for epoch 7: 0.23958891289758272\n",
      "Average running VALIDATION loss for epoch 7: 0.23413785941460552\n",
      "Average running TRAINING loss for epoch 8: 0.228589138144563\n",
      "Average running VALIDATION loss for epoch 8: 0.24424980580806732\n",
      "Average running TRAINING loss for epoch 9: 0.22887748447728568\n",
      "Average running VALIDATION loss for epoch 9: 0.23547068939489477\n",
      "Average running TRAINING loss for epoch 10: 0.21920400517510957\n",
      "Average running VALIDATION loss for epoch 10: 0.26972166142042947\n",
      "Average running TRAINING loss for epoch 11: 0.22016115428815627\n",
      "Average running VALIDATION loss for epoch 11: 0.23203961200573864\n",
      "Average running TRAINING loss for epoch 12: 0.21451813400048633\n",
      "Average running VALIDATION loss for epoch 12: 0.2500755291651277\n",
      "Average running TRAINING loss for epoch 13: 0.21048739475422892\n",
      "Average running VALIDATION loss for epoch 13: 0.23436083074878244\n",
      "Average running TRAINING loss for epoch 14: 0.21060257861069565\n",
      "Average running VALIDATION loss for epoch 14: 0.24954434440416448\n",
      "Average running TRAINING loss for epoch 15: 0.20483215615667147\n",
      "Average running VALIDATION loss for epoch 15: 0.22060252682251089\n",
      "Average running TRAINING loss for epoch 16: 0.19839009404953184\n",
      "Average running VALIDATION loss for epoch 16: 0.22360870110638\n",
      "Average running TRAINING loss for epoch 17: 0.20106437815160588\n",
      "Average running VALIDATION loss for epoch 17: 0.211518192115952\n",
      "Average running TRAINING loss for epoch 18: 0.19384957002154712\n",
      "Average running VALIDATION loss for epoch 18: 0.22860558681628285\n",
      "Average running TRAINING loss for epoch 19: 0.18904261082282353\n",
      "Average running VALIDATION loss for epoch 19: 0.2502511012203553\n",
      "Average running TRAINING loss for epoch 20: 0.1907999350143404\n",
      "Average running VALIDATION loss for epoch 20: 0.22746297278824976\n",
      "Average running TRAINING loss for epoch 21: 0.18919581206965036\n",
      "Average running VALIDATION loss for epoch 21: 0.24017004318097057\n",
      "Average running TRAINING loss for epoch 22: 0.18515173422879186\n",
      "Average running VALIDATION loss for epoch 22: 0.22802615297191284\n",
      "Average running TRAINING loss for epoch 23: 0.18295206238740477\n",
      "Average running VALIDATION loss for epoch 23: 0.2248931233497227\n",
      "Average running TRAINING loss for epoch 24: 0.18166227625875636\n",
      "Average running VALIDATION loss for epoch 24: 0.24133001530871673\n",
      "Average running TRAINING loss for epoch 25: 0.17379590332636546\n",
      "Average running VALIDATION loss for epoch 25: 0.22574074654018178\n",
      "Average running TRAINING loss for epoch 26: 0.1767023130287898\n",
      "Average running VALIDATION loss for epoch 26: 0.23650005898054907\n",
      "Average running TRAINING loss for epoch 27: 0.17461122694457398\n",
      "Average running VALIDATION loss for epoch 27: 0.22290250834296732\n",
      "Average running TRAINING loss for epoch 28: 0.16990100553837315\n",
      "Average running VALIDATION loss for epoch 28: 0.2433766468482859\n",
      "Average running TRAINING loss for epoch 29: 0.17088938167254472\n",
      "Average running VALIDATION loss for epoch 29: 0.22735345845713334\n",
      "Average running TRAINING loss for epoch 30: 0.16674974631389666\n",
      "Average running VALIDATION loss for epoch 30: 0.2266161941430148\n",
      "Average running TRAINING loss for epoch 31: 0.1690694777102306\n",
      "Average running VALIDATION loss for epoch 31: 0.23489992145229788\n",
      "Average running TRAINING loss for epoch 32: 0.17142541398262157\n",
      "Average running VALIDATION loss for epoch 32: 0.24138303364024444\n",
      "Average running TRAINING loss for epoch 33: 0.16243121901462818\n",
      "Average running VALIDATION loss for epoch 33: 0.21956975013017654\n",
      "Average running TRAINING loss for epoch 34: 0.16778248940305462\n",
      "Average running VALIDATION loss for epoch 34: 0.23256015164010665\n",
      "Average running TRAINING loss for epoch 35: 0.16921781562268734\n",
      "Average running VALIDATION loss for epoch 35: 0.25582362360814037\n",
      "Fold 6: 35 epochs, (0.16243121901462818,0.211518192115952)\n",
      "Average running TRAINING loss for epoch 1: 0.4337462042940074\n",
      "Average running VALIDATION loss for epoch 1: 0.3234458925092922\n",
      "Average running TRAINING loss for epoch 2: 0.3052521603631562\n",
      "Average running VALIDATION loss for epoch 2: 0.28211717570529266\n",
      "Average running TRAINING loss for epoch 3: 0.26967842253888474\n",
      "Average running VALIDATION loss for epoch 3: 0.27854571710614595\n",
      "Average running TRAINING loss for epoch 4: 0.25126015073780356\n",
      "Average running VALIDATION loss for epoch 4: 0.25546356334405784\n",
      "Average running TRAINING loss for epoch 5: 0.24001870413535628\n",
      "Average running VALIDATION loss for epoch 5: 0.23635472883196437\n",
      "Average running TRAINING loss for epoch 6: 0.23399849370892706\n",
      "Average running VALIDATION loss for epoch 6: 0.2383886496810352\n",
      "Average running TRAINING loss for epoch 7: 0.2228812638605977\n",
      "Average running VALIDATION loss for epoch 7: 0.22244788618648753\n",
      "Average running TRAINING loss for epoch 8: 0.2197024036070396\n",
      "Average running VALIDATION loss for epoch 8: 0.2204369232058525\n",
      "Average running TRAINING loss for epoch 9: 0.21414917476218323\n",
      "Average running VALIDATION loss for epoch 9: 0.21766489525051677\n",
      "Average running TRAINING loss for epoch 10: 0.2060859807092568\n",
      "Average running VALIDATION loss for epoch 10: 0.23082742051166646\n",
      "Average running TRAINING loss for epoch 11: 0.20450042181744657\n",
      "Average running VALIDATION loss for epoch 11: 0.21243392763768926\n",
      "Average running TRAINING loss for epoch 12: 0.1939161755144596\n",
      "Average running VALIDATION loss for epoch 12: 0.22509363205993876\n",
      "Average running TRAINING loss for epoch 13: 0.19782843607766876\n",
      "Average running VALIDATION loss for epoch 13: 0.25188228049698996\n",
      "Average running TRAINING loss for epoch 14: 0.18913304959905558\n",
      "Average running VALIDATION loss for epoch 14: 0.2225343909333734\n",
      "Average running TRAINING loss for epoch 15: 0.1864425265711957\n",
      "Average running VALIDATION loss for epoch 15: 0.25057436306686964\n",
      "Average running TRAINING loss for epoch 16: 0.18141020468339839\n",
      "Average running VALIDATION loss for epoch 16: 0.23437048582469716\n",
      "Average running TRAINING loss for epoch 17: 0.17961951544315652\n",
      "Average running VALIDATION loss for epoch 17: 0.2215510979294777\n",
      "Average running TRAINING loss for epoch 18: 0.1776477249403452\n",
      "Average running VALIDATION loss for epoch 18: 0.22751125079744003\n",
      "Average running TRAINING loss for epoch 19: 0.17480438009932123\n",
      "Average running VALIDATION loss for epoch 19: 0.25069331246263843\n",
      "Average running TRAINING loss for epoch 20: 0.17371330487317052\n",
      "Average running VALIDATION loss for epoch 20: 0.22366789684576147\n",
      "Average running TRAINING loss for epoch 21: 0.17090222046806894\n",
      "Average running VALIDATION loss for epoch 21: 0.23213136020828695\n",
      "Average running TRAINING loss for epoch 22: 0.1689291443868444\n",
      "Average running VALIDATION loss for epoch 22: 0.22931626789710102\n",
      "Average running TRAINING loss for epoch 23: 0.1657192924156271\n",
      "Average running VALIDATION loss for epoch 23: 0.2309590963756337\n",
      "Average running TRAINING loss for epoch 24: 0.16836568562249685\n",
      "Average running VALIDATION loss for epoch 24: 0.23041354645701015\n",
      "Average running TRAINING loss for epoch 25: 0.15827394418161492\n",
      "Average running VALIDATION loss for epoch 25: 0.24539537727832794\n",
      "Average running TRAINING loss for epoch 26: 0.16264758287961112\n",
      "Average running VALIDATION loss for epoch 26: 0.23554593149353475\n",
      "Average running TRAINING loss for epoch 27: 0.15659357254104367\n",
      "Average running VALIDATION loss for epoch 27: 0.22176570576779983\n",
      "Average running TRAINING loss for epoch 28: 0.1517625699975881\n",
      "Average running VALIDATION loss for epoch 28: 0.2455894789275001\n",
      "Average running TRAINING loss for epoch 29: 0.15429855851006918\n",
      "Average running VALIDATION loss for epoch 29: 0.2282185401110088\n",
      "Average running TRAINING loss for epoch 30: 0.14893431704619836\n",
      "Average running VALIDATION loss for epoch 30: 0.24074160745915243\n",
      "Average running TRAINING loss for epoch 31: 0.15182567840634748\n",
      "Average running VALIDATION loss for epoch 31: 0.22513293124297085\n",
      "Average running TRAINING loss for epoch 32: 0.14798377473549595\n",
      "Average running VALIDATION loss for epoch 32: 0.2637457667904742\n",
      "Fold 7: 32 epochs, (0.14798377473549595,0.21243392763768926)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from components.fn_classifier import FNDataset, FNClassifier\n",
    "\n",
    "k = 8 #5\n",
    "epochs_per_fold = 300 #50 #25 #30 #50\n",
    "dataset_name = \"FN_dataset\"\n",
    "#\"20230301_cross_eroded_equalized_cropped\" #\"20230301_cross_eroded_cropped\"\n",
    "#\"cross_excluded_eroded_144_offline_added\"\n",
    "#\"dataset_cross_no_BAK8095L_eroded_offline_low_plus\" #\"dataset_cross_no_BAK8095L_eroded_offline_512\" #\"dataset_cross_offline_eroded\" # \"dataset_cross_offline\"\n",
    "\n",
    "# Create dataset\n",
    "dataset = FNDataset(f\"F:/Data/ConeDatasets/{dataset_name}/train\", train=True)\n",
    "\n",
    "# Train WITH all k folds\n",
    "dataloaders = dataset.get_dataloaders_k_fold_cross_validation(k, batch_size=64)\n",
    "#FCN.train_networks_k_fold_cross_validation(dataloaders, max_epochs=epochs_per_fold, early_stopping=True)\n",
    "FNClassifier.train_networks_k_fold_cross_validation(dataloaders, max_epochs=epochs_per_fold, early_stopping=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('cones')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4332508b189ee8bdd2bfb6f37a5afa2b4445b5015b043b162e5ea53555c7652"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
